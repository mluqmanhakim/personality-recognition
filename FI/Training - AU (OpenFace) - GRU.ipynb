{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08538b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luqman/miniconda3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af9c0ac",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "336183ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/nas/project_data/B1_Behavior/hakim/FI/openface/fi_train_lld_au_bert.pkl'\n",
    "val_path = '/nas/project_data/B1_Behavior/hakim/FI/openface/fi_valid_lld_au_bert.pkl'\n",
    "test_path = '/nas/project_data/B1_Behavior/hakim/FI/openface/fi_test_lld_au_bert.pkl'\n",
    "        \n",
    "with open(train_path, 'rb') as f:\n",
    "    train_data_pickle = pickle.load(f)\n",
    "    train_ids, train_data, train_label = zip(*train_data_pickle)\n",
    "    _, au_train, _ = zip(*train_data)\n",
    "    \n",
    "with open(val_path, 'rb') as f:\n",
    "    val_data_pickle = pickle.load(f)\n",
    "    val_ids, val_data, val_label = zip(*val_data_pickle)\n",
    "    _, au_val, _ = zip(*val_data)\n",
    "    \n",
    "with open(test_path, 'rb') as f:\n",
    "    test_data_pickle = pickle.load(f)\n",
    "    test_ids, test_data, test_label = zip(*test_data_pickle)\n",
    "    _, au_test, _ = zip(*test_data)\n",
    "\n",
    "with open('/home/luqman/linear_mult/linear_mult/mean_std_fi.pkl', 'rb') as f:\n",
    "    mean_std = pickle.load(f)\n",
    "    \n",
    "train_features = []\n",
    "train_labels = []\n",
    "val_features = []\n",
    "val_labels = []\n",
    "test_features = []\n",
    "test_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e2c1f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, id in enumerate(train_ids):\n",
    "    label = train_label[i]\n",
    "    au = (au_train[i] - mean_std[1][0]) / mean_std[1][1]\n",
    "#     padding to 450x35\n",
    "    if au.shape[0] < 450:\n",
    "        au = np.pad(au, ((0,450-au.shape[0]),(0,0)), 'constant', constant_values=0)\n",
    "    else:\n",
    "        au = au[:450,:]\n",
    "#         include only several AU\n",
    "    au = np.concatenate((au[:,17:20], au[:,21:23], au[:,24:29], au[:,30:31]), axis=1)\n",
    "    train_features.append(au)\n",
    "    train_labels.append(label)\n",
    "    \n",
    "for i, id in enumerate(val_ids):\n",
    "    label = train_label[i]\n",
    "    au = (au_val[i] - mean_std[1][0]) / mean_std[1][1]\n",
    "    if au.shape[0] < 450:\n",
    "        au = np.pad(au, ((0,450-au.shape[0]),(0,0)), 'constant', constant_values=0)\n",
    "    else:\n",
    "        au = au[:450,:]\n",
    "    #         include only several AU\n",
    "    au = np.concatenate((au[:,17:20], au[:,21:23], au[:,24:29], au[:,30:31]), axis=1)\n",
    "    val_features.append(au)\n",
    "    val_labels.append(label)\n",
    "    \n",
    "for i, id in enumerate(test_ids):\n",
    "    label = train_label[i]\n",
    "    au = (au_test[i] - mean_std[1][0]) / mean_std[1][1]\n",
    "    if au.shape[0] < 450:\n",
    "        au = np.pad(au, ((0,450-au.shape[0]),(0,0)), 'constant', constant_values=0)\n",
    "    else:\n",
    "        au = au[:450,:]\n",
    "    #         include only several AU\n",
    "    au = np.concatenate((au[:,17:20], au[:,21:23], au[:,24:29], au[:,30:31]), axis=1)\n",
    "    test_features.append(au)\n",
    "    test_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab64d34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = torch.Tensor(np.array(train_features))\n",
    "train_labels = torch.Tensor(np.array(train_labels))\n",
    "val_features = torch.Tensor(np.array(val_features))\n",
    "val_labels = torch.Tensor(np.array(val_labels))\n",
    "test_features = torch.Tensor(np.array(test_features))\n",
    "test_labels = torch.Tensor(np.array(test_labels))\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "train_ds = TensorDataset(train_features, train_labels)\n",
    "val_ds = TensorDataset(val_features, val_labels)\n",
    "test_ds = TensorDataset(test_features, test_labels)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbb63fd",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "292f1947",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "893464e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, bidirectional, output_dim, dropout, num_layers, batch_size):\n",
    "        super(GRU, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "        self.gru = torch.nn.GRU(input_size=self.input_size, \n",
    "                                hidden_size=self.hidden_size,\n",
    "                                num_layers=self.num_layers,\n",
    "                                dropout=self.dropout,\n",
    "                                bidirectional=bidirectional,\n",
    "                                batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_size if not bidirectional else hidden_size * 2, output_dim)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, h = self.gru(x)\n",
    "        out = self.relu(out[:,-1,:])\n",
    "        out = self.fc(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3436be9a",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "334eaa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, epochs, patience=5):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_maes = []\n",
    "    last_loss = 0.000001\n",
    "    trigger_times = 0\n",
    "\n",
    "    for epoch in range(0, epochs):\n",
    "        model.train()\n",
    "        batch_losses = []\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            preds = model(inputs.to(device))\n",
    "            loss = loss_fn(preds, targets.to(device))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_losses.append(loss.item())\n",
    "\n",
    "        train_loss = np.mean(batch_losses)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_val_losses = []\n",
    "            batch_val_mae = []\n",
    "\n",
    "            for inputs, targets in val_loader:\n",
    "                model.eval()\n",
    "                preds = model(inputs.to(device))\n",
    "                loss = loss_fn(preds, targets.to(device))\n",
    "                batch_val_losses.append(loss.item())\n",
    "                val_mae = mae(preds, targets.to(device))\n",
    "                batch_val_mae.append(val_mae.item())\n",
    "            \n",
    "            val_loss = np.mean(batch_val_losses)\n",
    "            val_mae = np.mean(batch_val_mae)\n",
    "            scheduler.step(val_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            val_maes.append(val_mae)\n",
    "  \n",
    "        print(f'Epoch {epoch} - train loss: {train_loss:.4f} - val loss: {val_loss:.4f} - val 1-mae: {(1-val_mae):.4f}')\n",
    "        \n",
    "        if val_loss > last_loss:\n",
    "            trigger_times += 1\n",
    "            print('Trigger:', trigger_times)\n",
    "            if trigger_times >= patience:\n",
    "                print('Early stop')\n",
    "                break\n",
    "        else:\n",
    "            print('Trigger: 0')\n",
    "            trigger_times = 0\n",
    "        last_loss = val_loss\n",
    "        \n",
    "    return train_losses, val_losses, val_maes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a41f5ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRU(input_size=11, \n",
    "                  hidden_size=256, \n",
    "                  bidirectional=True, \n",
    "                  output_dim=5, \n",
    "                  dropout=0.2, \n",
    "                  num_layers=2, \n",
    "                  batch_size=BATCH_SIZE)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "LR = 0.00001\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "mae = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c69f5496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - train loss: 0.0239 - val loss: 0.0242 - val 1-mae: 0.8742\n",
      "Trigger: 1\n",
      "Epoch 1 - train loss: 0.0236 - val loss: 0.0240 - val 1-mae: 0.8749\n",
      "Trigger: 0\n",
      "Epoch 2 - train loss: 0.0234 - val loss: 0.0238 - val 1-mae: 0.8754\n",
      "Trigger: 0\n",
      "Epoch 3 - train loss: 0.0231 - val loss: 0.0236 - val 1-mae: 0.8761\n",
      "Trigger: 0\n",
      "Epoch 4 - train loss: 0.0230 - val loss: 0.0235 - val 1-mae: 0.8764\n",
      "Trigger: 0\n",
      "Epoch 5 - train loss: 0.0228 - val loss: 0.0234 - val 1-mae: 0.8768\n",
      "Trigger: 0\n",
      "Epoch 6 - train loss: 0.0226 - val loss: 0.0232 - val 1-mae: 0.8772\n",
      "Trigger: 0\n",
      "Epoch 7 - train loss: 0.0225 - val loss: 0.0232 - val 1-mae: 0.8774\n",
      "Trigger: 0\n",
      "Epoch 8 - train loss: 0.0224 - val loss: 0.0230 - val 1-mae: 0.8779\n",
      "Trigger: 0\n",
      "Epoch 9 - train loss: 0.0221 - val loss: 0.0230 - val 1-mae: 0.8780\n",
      "Trigger: 0\n",
      "Epoch 10 - train loss: 0.0220 - val loss: 0.0231 - val 1-mae: 0.8780\n",
      "Trigger: 1\n",
      "Epoch 11 - train loss: 0.0219 - val loss: 0.0230 - val 1-mae: 0.8783\n",
      "Trigger: 0\n",
      "Epoch 12 - train loss: 0.0219 - val loss: 0.0229 - val 1-mae: 0.8786\n",
      "Trigger: 0\n",
      "Epoch 13 - train loss: 0.0218 - val loss: 0.0229 - val 1-mae: 0.8785\n",
      "Trigger: 1\n",
      "Epoch 14 - train loss: 0.0218 - val loss: 0.0230 - val 1-mae: 0.8784\n",
      "Trigger: 2\n",
      "Epoch 15 - train loss: 0.0217 - val loss: 0.0230 - val 1-mae: 0.8784\n",
      "Trigger: 0\n",
      "Epoch 16 - train loss: 0.0216 - val loss: 0.0231 - val 1-mae: 0.8782\n",
      "Trigger: 1\n",
      "Epoch 17 - train loss: 0.0216 - val loss: 0.0232 - val 1-mae: 0.8780\n",
      "Trigger: 2\n",
      "Epoch 18 - train loss: 0.0216 - val loss: 0.0232 - val 1-mae: 0.8781\n",
      "Trigger: 0\n",
      "Epoch 19 - train loss: 0.0216 - val loss: 0.0232 - val 1-mae: 0.8779\n",
      "Trigger: 1\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses, val_maes = train(model, train_loader, val_loader, epochs=20, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077fce27",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d40309a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7feccf01d280>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(train_losses, label='Train loss')\n",
    "ax.plot(val_losses, label='Val loss')\n",
    "ax.plot(val_maes, label='Val MAE')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "plt.title(\"Training result\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc68b22",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "07dc934e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-MAE: 0.8761088539447103\n",
      "1-MAE Extraversion: 0.8802891267197472\n",
      "1-MAE Neuroticism: 0.8691829762288502\n",
      "1-MAE Agreeableness: 0.8718050739594868\n",
      "1-MAE Conscientiousness: 0.887361860701016\n",
      "1-MAE Openness: 0.8719052321144513\n",
      "r2_extraversion: -22.279857163574167\n",
      "r2_neuroticism: -23.52080221602617\n",
      "r2_agreeableness: -13.18765170823146\n",
      "r2_conscientiousness: -38.74664432132872\n",
      "r2_openness: -22.208853656557146\n",
      "r2: -23.988761813143533\n"
     ]
    }
   ],
   "source": [
    "def test(model, test_loader):\n",
    "    mae = torch.nn.L1Loss()\n",
    "    predictions = np.array([])\n",
    "    labels = np.array([])\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        batch_loss_ext = []\n",
    "        batch_loss_neu = []\n",
    "        batch_loss_agr = []\n",
    "        batch_loss_con = []\n",
    "        batch_loss_ope = []\n",
    "\n",
    "        for inputs, targets in test_loader:\n",
    "            batch_preds = model(inputs.to(device))\n",
    "            loss_ext = mae(batch_preds[:,0], targets[:,0].to(device))\n",
    "            loss_neu = mae(batch_preds[:,1], targets[:,1].to(device))\n",
    "            loss_agr = mae(batch_preds[:,2], targets[:,2].to(device))\n",
    "            loss_con = mae(batch_preds[:,3], targets[:,3].to(device))\n",
    "            loss_ope = mae(batch_preds[:,4], targets[:,4].to(device))\n",
    "            batch_loss_ext.append(loss_ext.item())\n",
    "            batch_loss_neu.append(loss_neu.item())\n",
    "            batch_loss_agr.append(loss_agr.item())\n",
    "            batch_loss_con.append(loss_con.item())\n",
    "            batch_loss_ope.append(loss_ope.item())\n",
    "            predictions = np.vstack((predictions, batch_preds.cpu())) if len(predictions) != 0 else batch_preds.cpu()\n",
    "            labels = np.vstack((labels, targets.cpu())) if len(labels) != 0 else targets.cpu()\n",
    "\n",
    "        loss_ext = 1 - np.mean(batch_loss_ext)\n",
    "        loss_neu = 1 - np.mean(batch_loss_neu)\n",
    "        loss_agr = 1 - np.mean(batch_loss_agr)\n",
    "        loss_con = 1 - np.mean(batch_loss_con)\n",
    "        loss_ope = 1 - np.mean(batch_loss_ope)\n",
    "        loss = (loss_ext + loss_neu + loss_agr + loss_con + loss_ope) / 5\n",
    "\n",
    "        print(f'1-MAE: {loss:.4f}')\n",
    "        print(f'1-MAE O: {loss_ope:.4f}')\n",
    "        print(f'1-MAE C: {loss_con:.4f}')\n",
    "        print(f'1-MAE E: {loss_ext:.4f}')\n",
    "        print(f'1-MAE A: {loss_agr:.4f}')\n",
    "        print(f'1-MAE N: {loss_neu:.4f}')\n",
    "        \n",
    "        spearman_r_ext = stats.spearmanr(predictions[:,0], labels[:,0])\n",
    "        spearman_r_neu = stats.spearmanr(predictions[:,1], labels[:,1])\n",
    "        spearman_r_agr = stats.spearmanr(predictions[:,2], labels[:,2])\n",
    "        spearman_r_con = stats.spearmanr(predictions[:,3], labels[:,3])\n",
    "        spearman_r_ope = stats.spearmanr(predictions[:,4], labels[:,4])\n",
    "        print(f'O - R: {spearman_r_ope[0]:.3f} - p: {spearman_r_ope[1]:.3f}')\n",
    "        print(f'C - R: {spearman_r_con[0]:.3f} - p: {spearman_r_con[1]:.3f}')\n",
    "        print(f'E - R: {spearman_r_ext[0]:.3f} - p: {spearman_r_ext[1]:.3f}')\n",
    "        print(f'A - R: {spearman_r_agr[0]:.3f} - p: {spearman_r_agr[1]:.3f}')\n",
    "        print(f'N - R: {spearman_r_neu[0]:.3f} - p: {spearman_r_neu[1]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624e1b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5681366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline prior 1-mae: 0.8791207298636436\n"
     ]
    }
   ],
   "source": [
    "prior = test_labels.mean(axis=0)\n",
    "test_prior = torch.from_numpy(np.repeat(np.expand_dims(prior, axis=0), 2000, axis=0))\n",
    "value = mae(test_prior, test_labels)\n",
    "print('baseline prior 1-mae:', 1-value.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b8f539",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
